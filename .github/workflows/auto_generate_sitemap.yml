name: Auto Generate Sitemap

on:
  push:
    branches: [ main, master ]
    paths:
      - '**.html'
      - '**.md'
      - '**.php'
      - '**.htm'
      - 'docs/**'
      - 'content/**'
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:
    inputs:
      force_regenerate:
        description: 'Force regenerate sitemap even if no changes'
        required: false
        default: 'false'
        type: choice
        options:
          - 'true'
          - 'false'
      base_url:
        description: 'Base URL override (optional)'
        required: false
        type: string
  schedule:
    # Run weekly on Sundays at 02:00 UTC
    - cron: '0 2 * * 0'

jobs:
  generate-sitemap:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0  # Fetch all history for accurate last modified dates
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Determine base URL
        id: base_url
        run: |
          # Priority order:
          # 1. Manual input
          # 2. Repository environment variable  
          # 3. Auto-detect from repository
          
          BASE_URL="${{ github.event.inputs.base_url }}"
          
          if [ -z "$BASE_URL" ]; then
            # Try environment variable
            BASE_URL="${{ vars.SITE_BASE_URL }}"
          fi
          
          if [ -z "$BASE_URL" ]; then
            # Auto-detect GitHub Pages URL
            REPO_NAME="${{ github.repository }}"
            REPO_OWNER="${{ github.repository_owner }}"
            
            # Check if it's a user/org page (repo name matches username)
            if [[ "$REPO_NAME" == "$REPO_OWNER/$REPO_OWNER.github.io" ]]; then
              BASE_URL="https://$REPO_OWNER.github.io"
            else
              # Project page
              PROJECT_NAME="${REPO_NAME#*/}"  # Remove owner/ prefix
              BASE_URL="https://$REPO_OWNER.github.io/$PROJECT_NAME"
            fi
          fi
          
          echo "Detected base URL: $BASE_URL"
          echo "base_url=$BASE_URL" >> $GITHUB_OUTPUT
      
      - name: Create sitemap configuration
        run: |
          cat > sitemap-config.json << 'EOF'
          {
            "file_extensions": [".html", ".htm", ".php", ".asp", ".aspx", ".md", ".markdown", ".pdf"],
            "exclude_patterns": [
              ".*\\.git.*",
              ".*node_modules.*",
              ".*\\.venv.*", 
              ".*__pycache__.*",
              ".*\\.DS_Store.*",
              ".*\\.tmp.*",
              ".*404\\.html$",
              ".*error\\.html$",
              ".*\\.github.*",
              ".*README\\.md$"
            ],
            "include_patterns": [],
            "priority_rules": {
              "index": 1.0,
              "home": 1.0,
              "main": 0.9,
              "about": 0.8,
              "contact": 0.8,
              "blog": 0.7,
              "post": 0.6,
              "page": 0.5,
              "default": 0.5
            },
            "changefreq_rules": {
              "index": "daily",
              "blog": "weekly",
              "post": "weekly",
              "news": "daily",
              "static": "monthly", 
              "default": "weekly"
            },
            "auto_detect": {
              "readme_as_index": true,
              "detect_project_type": true,
              "scan_html_links": true,
              "include_markdown": true
            }
          }
          EOF
      
      - name: Create scripts directory
        run: |
          mkdir -p scripts
      
      - name: Create sitemap generator scripts
        run: |
          # Create the main generator script
          cat > .github/scripts/generate_sitemap.py << 'GENERATOR_EOF'
          #!/usr/bin/env python3
          import os, sys, json, argparse, subprocess, re
          from datetime import datetime
          import xml.etree.ElementTree as ET
          
          class SitemapGenerator:
              def __init__(self, base_url, config):
                  self.base_url = base_url.rstrip('/')
                  self.config = config
                  self.urls = []
              
              def get_git_last_modified(self, file_path):
                  try:
                      result = subprocess.run(['git', 'log', '-1', '--format=%ct', '--', file_path], 
                                            capture_output=True, text=True, check=True)
                      if result.stdout.strip():
                          timestamp = int(result.stdout.strip())
                          return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%dT%H:%M:%S+00:00')
                  except: pass
                  return None
              
              def get_file_priority(self, file_path):
                  priority_rules = self.config.get('priority_rules', {})
                  file_name = os.path.basename(file_path).lower()
                  name_without_ext = os.path.splitext(file_name)[0]
                  
                  for keyword, priority in priority_rules.items():
                      if keyword != 'default' and (keyword in name_without_ext or keyword in file_path.lower()):
                          return priority
                  return priority_rules.get('default', 0.5)
              
              def get_change_frequency(self, file_path):
                  changefreq_rules = self.config.get('changefreq_rules', {})
                  for keyword, freq in changefreq_rules.items():
                      if keyword != 'default' and keyword in file_path.lower():
                          return freq
                  return changefreq_rules.get('default', 'weekly')
              
              def should_exclude_file(self, file_path):
                  exclude_patterns = self.config.get('exclude_patterns', [])
                  for pattern in exclude_patterns:
                      regex_pattern = pattern.replace('.*', '.*').replace('*', '[^/]*')
                      if re.search(regex_pattern, file_path):
                          return True
                  return False
              
              def convert_path_to_url(self, file_path):
                  url_path = file_path.replace('\\', '/').lstrip('./')
                  
                  if url_path == 'README.md' and self.config.get('auto_detect', {}).get('readme_as_index', True):
                      url_path = ''
                  elif url_path.endswith('/index.html') or url_path.endswith('/index.htm'):
                      url_path = url_path.rsplit('/index.', 1)[0] + '/'
                  elif url_path == 'index.html' or url_path == 'index.htm':
                      url_path = ''
                  elif url_path.endswith('.md'):
                      url_path = url_path[:-3] + '.html'
                  
                  if url_path and not url_path.startswith('/'):
                      url_path = '/' + url_path
                  elif not url_path:
                      url_path = '/'
                  
                  return self.base_url + url_path
              
              def scan_directory(self):
                  extensions = self.config.get('file_extensions', ['.html', '.htm', '.php', '.md'])
                  print(f"ğŸ” Scanning for files with extensions: {', '.join(extensions)}")
                  
                  for root, dirs, files in os.walk('.'):
                      dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['node_modules', '__pycache__', '.venv', 'venv', '.git']]
                      
                      for file in files:
                          file_path = os.path.join(root, file)
                          relative_path = os.path.relpath(file_path, '.')
                          
                          if not any(file.lower().endswith(ext) for ext in extensions):
                              continue
                          if self.should_exclude_file(relative_path):
                              continue
                          
                          url = self.convert_path_to_url(relative_path)
                          lastmod = self.get_git_last_modified(relative_path)
                          priority = self.get_file_priority(relative_path)
                          changefreq = self.get_change_frequency(relative_path)
                          
                          if lastmod is None:
                              try:
                                  mtime = os.path.getmtime(file_path)
                                  lastmod = datetime.fromtimestamp(mtime).strftime('%Y-%m-%dT%H:%M:%S+00:00')
                              except OSError:
                                  lastmod = datetime.now().strftime('%Y-%m-%dT%H:%M:%S+00:00')
                          
                          self.urls.append({
                              'loc': url, 'lastmod': lastmod, 'changefreq': changefreq,
                              'priority': priority, 'file_path': relative_path
                          })
                  
                  print(f"ğŸ“Š Found {len(self.urls)} URLs to include in sitemap")
              
              def generate_xml(self):
                  urlset = ET.Element('urlset')
                  urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
                  
                  self.urls.sort(key=lambda x: (-x['priority'], x['loc']))
                  
                  for url_data in self.urls:
                      url_elem = ET.SubElement(urlset, 'url')
                      
                      loc_elem = ET.SubElement(url_elem, 'loc')
                      loc_elem.text = url_data['loc']
                      
                      if url_data.get('lastmod'):
                          lastmod_elem = ET.SubElement(url_elem, 'lastmod')
                          lastmod_elem.text = url_data['lastmod']
                      
                      if url_data.get('changefreq'):
                          changefreq_elem = ET.SubElement(url_elem, 'changefreq')
                          changefreq_elem.text = url_data['changefreq']
                      
                      if url_data.get('priority') is not None:
                          priority_elem = ET.SubElement(url_elem, 'priority')
                          priority_elem.text = f"{url_data['priority']:.1f}"
                  
                  return urlset
              
              def write_sitemap(self, output_file):
                  if not self.urls:
                      print("âš ï¸ No URLs found to include in sitemap")
                      return False
                  
                  xml_root = self.generate_xml()
                  xml_str = '<?xml version="1.0" encoding="UTF-8"?>\n'
                  xml_str += ET.tostring(xml_root, encoding='unicode', method='xml')
                  
                  # Basic formatting
                  xml_str = xml_str.replace('><', '>\n<')
                  lines = xml_str.split('\n')
                  formatted_lines = []
                  indent_level = 0
                  
                  for line in lines:
                      if line.strip():
                          if line.strip().startswith('</') and not line.strip().startswith('</url>'):
                              indent_level -= 1
                          formatted_lines.append('  ' * indent_level + line.strip())
                          if (line.strip().startswith('<') and not line.strip().startswith('</') and 
                              not line.strip().endswith('/>') and 
                              not any(tag in line for tag in ['<loc>', '<lastmod>', '<changefreq>', '<priority>'])):
                              indent_level += 1
                  
                  try:
                      with open(output_file, 'w', encoding='utf-8') as f:
                          f.write('\n'.join(formatted_lines))
                      print(f"âœ… Sitemap written to {output_file}")
                      return True
                  except IOError as e:
                      print(f"âŒ Error writing sitemap: {e}")
                      return False
          
          def main():
              parser = argparse.ArgumentParser(description='Generate sitemap for website')
              parser.add_argument('base_url', help='Base URL for the sitemap')
              parser.add_argument('--config', required=True, help='Configuration file path')
              parser.add_argument('--output', default='sitemap.xml', help='Output sitemap file')
              parser.add_argument('--verbose', action='store_true', help='Verbose output')
              
              args = parser.parse_args()
              
              try:
                  with open(args.config, 'r') as f:
                      config = json.load(f)
              except (FileNotFoundError, json.JSONDecodeError) as e:
                  print(f"âŒ Error loading config: {e}")
                  sys.exit(1)
              
              if args.verbose:
                  print(f"ğŸŒ Base URL: {args.base_url}")
                  print(f"ğŸ“ Output file: {args.output}")
              
              generator = SitemapGenerator(args.base_url, config)
              generator.scan_directory()
              
              if generator.write_sitemap(args.output):
                  print(f"ğŸ‰ Successfully generated sitemap with {len(generator.urls)} URLs")
                  if args.verbose and generator.urls:
                      print("\nğŸ“‹ Sample URLs:")
                      for url_data in generator.urls[:10]:
                          print(f"   {url_data['loc']} (priority: {url_data['priority']:.1f})")
                      if len(generator.urls) > 10:
                          print(f"   ... and {len(generator.urls) - 10} more URLs")
              else:
                  sys.exit(1)
          
          if __name__ == '__main__':
              main()
          GENERATOR_EOF
          
          # Create the conditional checker script
          cat > .gtihub/scripts/update_sitemap_if_needed.py << 'CHECKER_EOF'
          #!/usr/bin/env python3
          import os, sys, json, argparse, subprocess
          from datetime import datetime
          
          def get_git_last_modified(file_path):
              try:
                  result = subprocess.run(['git', 'log', '-1', '--format=%ct', '--', file_path], 
                                        capture_output=True, text=True, check=True)
                  if result.stdout.strip():
                      return int(result.stdout.strip())
              except: pass
              return None
          
          def find_content_files(config):
              content_files = []
              extensions = config.get('file_extensions', ['.html', '.htm', '.php', '.md'])
              exclude_patterns = config.get('exclude_patterns', [])
              
              for root, dirs, files in os.walk('.'):
                  dirs[:] = [d for d in dirs if not d.startswith('.') and 'node_modules' not in d]
                  for file in files:
                      file_path = os.path.relpath(os.path.join(root, file), '.')
                      if (any(file.lower().endswith(ext) for ext in extensions) and
                          not any(pattern.replace('.*', '') in file_path for pattern in exclude_patterns)):
                          content_files.append(file_path)
              return content_files
          
          def main():
              parser = argparse.ArgumentParser()
              parser.add_argument('--base-url', required=True)
              parser.add_argument('--output', default='sitemap.xml')
              parser.add_argument('--config', required=True)
              args = parser.parse_args()
              
              try:
                  with open(args.config, 'r') as f:
                      config = json.load(f)
              except Exception as e:
                  print(f"âŒ Error loading config: {e}")
                  sys.exit(1)
              
              # Always proceed to main generation - let the main script handle the logic
              print("ğŸ”„ Proceeding with sitemap generation...")
              
              # Call main generator
              try:
                  result = subprocess.run([
                      sys.executable, 'generate_sitemap.py', args.base_url,
                      '--config', args.config, '--output', args.output, '--verbose'
                  ], check=True)
                  print("âœ… Sitemap generation completed")
              except subprocess.CalledProcessError as e:
                  print(f"âŒ Generation failed: {e}")
                  sys.exit(1)
          
          if __name__ == '__main__':
              main()
          CHECKER_EOF
          
          chmod +x .github/scripts/generate_sitemap.py
          chmod +x .github/scripts/update_sitemap_if_needed.py
